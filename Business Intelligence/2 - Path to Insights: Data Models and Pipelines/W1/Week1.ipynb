{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Data Models and Pipelines\n",
    "\n",
    "## Get Started with Data Modeling, Schemas, and Databases\n",
    "\n",
    "###  Overview\n",
    "\n",
    "\n",
    "First, you'll learn about\n",
    "design patterns and database schemas,\n",
    "including common structures that BI professionals use.\n",
    "You'll also be introduced to\n",
    "data pipelines and ETL processes.\n",
    "You've learned that ETL stands for\n",
    "extract, transform, and load.\n",
    "This refers to the process of\n",
    "gathering data from source systems,\n",
    "converting it into a useful format,\n",
    "and bringing it into a data warehouse\n",
    "or other unified destination system. \n",
    "\n",
    "You'll also develop strategies\n",
    "for gathering information from stakeholders\n",
    "in order to help you develop\n",
    "more useful tools and processes for your team. \n",
    "\n",
    "After that, you'll focus\n",
    "on database optimization to reduce\n",
    "response time or the time it takes\n",
    "for a database to complete a user request.\n",
    "This will include exploring different types of\n",
    "databases and the five factors of database performance,\n",
    "workload, throughput,\n",
    "resources, optimization, and contention. \n",
    "\n",
    "Finally, you'll learn about the importance\n",
    "of quality testing your ETL processes,\n",
    "validating your database schema,\n",
    "and verifying business rules. \n",
    "\n",
    "\n",
    "### Modeling and Schemas\n",
    "\n",
    "\n",
    "In order to make databases useful,\n",
    "the data has to be organized.\n",
    "This includes both source systems\n",
    "from which data is ingested and\n",
    "moved and the destination\n",
    "database where it will be acted upon.\n",
    "These source systems could include data lakes,\n",
    "which are database systems that store large amounts of\n",
    "raw data in its original format until it's needed. \n",
    "\n",
    "\n",
    "Another type of source system is\n",
    "an Online Transaction Processing or OLTP database.\n",
    "An OLTP database is one that has been\n",
    "optimized for data processing instead of analysis.\n",
    "One type of destination system is a data mart,\n",
    "which is a subject oriented database that\n",
    "can be a subset of a larger data warehouse.\n",
    "Another possibility is using\n",
    "an Online Analytical Processing or OLAP database.\n",
    "This is a tool that has been\n",
    "optimized for analysis in addition to\n",
    "processing and can analyze data from multiple databases. \n",
    "\n",
    "Unstructured data is not organized\n",
    "in any easily identifiable manner.\n",
    "Structure data has been organized in a certain format,\n",
    "such as rows and columns. \n",
    "\n",
    "a data model is a tool for organizing\n",
    "data elements and how they relate to one another.\n",
    "These are conceptual models that help\n",
    "keep data consistent across the system. \n",
    "\n",
    "\n",
    "In order to create the data model,\n",
    "BI professionals will often use\n",
    "what is referred to as a design pattern.\n",
    "Design pattern is a solution that uses relevant measures\n",
    "and facts to create a model to support business needs.\n",
    "Think of it like a re-usable problem-solving template,\n",
    "which may be applied to many different scenarios. \n",
    "\n",
    "As a refresher, a schema is a way of\n",
    "describing how something such as data is organized.\n",
    "You may have encountered schemas\n",
    "before while working with databases.\n",
    "For example, some common schemas you\n",
    "might be familiar with include relational models,\n",
    "star schemas, snowflake schemas and noSQL schemas. \n",
    "\n",
    "### Dimensional Models\n",
    "\n",
    "A primary key is an identifier in a database that references a column or\n",
    "a group of columns in which each row uniquely identifies each record in the table.\n",
    "In this database we have primary keys in each table. \n",
    "\n",
    "A foreign key is a field within a database table that's a primary key in\n",
    "another table.\n",
    "The primary keys from each table also appear as foreign keys in other tables. \n",
    "\n",
    "\n",
    "A dimensional model is a type of relational model that has been optimized\n",
    "to quickly retrieve data from a data warehouse.\n",
    "Dimensional models can be broken down into facts for measurement and\n",
    "dimensions that add attributes for context. In a dimensional model,\n",
    "a fact is a measurement or metric.\n",
    "For example a monthly sales number could be a fact and a dimension is a piece of\n",
    "information that provides more detail and context regarding that fact.\n",
    "It's the who, what, where, when, why and how.\n",
    "So if our monthly sales number is the fact then the dimensions could be\n",
    "information about each sale, including the customer, the store location and\n",
    "what products were sold.\n",
    "\n",
    "An attribute is a characteristic or quality of data\n",
    "used to label the table columns. In dimensional models,\n",
    "attributes work kind of the same way. An attribute is a characteristic or quality\n",
    "that can be used to describe a dimension.\n",
    "So a dimension provides information about a fact and\n",
    "an attribute provides information about a dimension. \n",
    "\n",
    "Think about a passport.\n",
    "One dimension on your passport is your hair and eye color.\n",
    "If you have brown hair and eyes,\n",
    "brown is the attribute that describes that dimension. \n",
    "\n",
    "It's time for the dimensional model to use these\n",
    "things to create two types of tables: fact tables and dimension tables.\n",
    "A fact table contains measurements or metrics related to a particular event. \n",
    "\n",
    "A dimension table is where attributes of the dimensions of a fact are stored.\n",
    "These tables are joined the appropriate fact table using the foreign key.\n",
    "This gives meaning and context to the facts.\n",
    "That's how tables are connected in the dimensional model. \n",
    "\n",
    "Think of the schema like a blueprint, it doesn't hold data itself, but\n",
    "describes the shape of the data and how it might relate to other tables or models.\n",
    "Any entry in the database is an instance of that schema and\n",
    "will contain all of the properties described in the schema.\n",
    "\n",
    "- A star schema is a schema consisting of one fact table that references any\n",
    "number of dimension tables.\n",
    "As its name suggests, this schema is shaped like a star.\n",
    "Notice how each of the dimension tables is connected to the fact table at the center. \n",
    "\n",
    "- A snowflake schema is an extension of a star schema with additional dimensions\n",
    "and, often, subdimensions.\n",
    "These dimensions and subdimensions break down the schema into even more specific\n",
    "tables, creating a snowflake pattern. \n",
    "\n",
    "### Types of Schema\n",
    "\n",
    "- Star Schema\n",
    "\n",
    "A star schema is a schema consisting of one or more fact tables referencing any number of dimension tables. As its name suggests, this schema is shaped like a star. This type of schema is ideal for high-scale information delivery and makes read output more efficient. It also classifies attributes into facts and descriptive dimension attributes (product ID, customer name, sale date).\n",
    "\n",
    "All the dimension tables link back to the sales_fact table at the center, which confirms this is a star schema.\n",
    "\n",
    "- Snowflake Schema\n",
    "\n",
    "A snowflake schema is an extension of a star schema with additional dimensions and, often, subdimensions. \n",
    "\n",
    "This fact table branches out to multiple dimension tables and even subdimensions. The dimension tables break out multiple details, such as player international and player club stats, transfer history, and more.\n",
    "\n",
    "- Flat Model\n",
    "\n",
    "Flattened schemas are extremely simple database systems with a single table in which each record is represented by a single row of data. The rows are separated by a delimiter, like a column, to indicate the separations between records. Flat models are not relational; they can’t capture relationships between tables or data items. Because of this, flat models are more often used as a potential source within a data system to capture less complex data that doesn’t need to be updated.\n",
    "\n",
    "Think of a names col, and time col, that's it that's all. Like a single DF\n",
    "\n",
    "- Semi-Structured Schema\n",
    "\n",
    "In addition to traditional, relational schemas, there are also semi-structured database schemas which have much more flexible rules, but still maintain some organization. Because these databases have less rigid organizational rules, they are extremely flexible and are designed to quickly access data.\n",
    "\n",
    "1. Document schemas store data as documents, similar to JSON files. These documents store pairs of fields and values of different data types.\n",
    "\n",
    "2. Key-value schemas pair a string with some relationship to the data, like a filename or a URL, which is then used as a key. This key is connected to the data, which is stored in a single collection. Users directly request data by using the key to retrieve it.\n",
    "\n",
    "3. Wide-column schemas use flexible, scalable tables. Each row contains a key and related columns stored in a wide format.\n",
    "\n",
    "4. Graph schemas  store data items in collections called nodes. These nodes are connected by edges, which store information about how the nodes are related. However, unlike relational databases, these relationships change as new data is introduced into the nodes.\n",
    "\n",
    "### Database Comparison\n",
    "\n",
    "In particular, databases vary based on how the data is processed,\n",
    "organized and stored. \n",
    "\n",
    "A database migration involves moving data from one source platform to another target\n",
    "database.\n",
    "During a migration users transition the current database schemas,\n",
    "to a new desired state.\n",
    "This could involve adding tables or columns, splitting fields,\n",
    "removing elements, changing data types or other improvements.\n",
    "The database migration process often requires numerous phases and iterations,\n",
    "as well as lots of testing. \n",
    "\n",
    "OLTP,\n",
    "OLAP, Row-based, columnar, distributed, single-homed,\n",
    "separated storage and compute and combined databases. \n",
    "\n",
    "- OLTP\n",
    "\n",
    "OLTP databases managed database modification and\n",
    "are operated with traditional database management system software.\n",
    "These systems are designed to effectively store transactions and\n",
    "help ensure consistency. \n",
    "\n",
    "If two people add the same book to their cart, but there's only one copy then\n",
    "the person who completes the checkout process first will get the book.\n",
    "And the OLTP system ensures that there aren't more copies sold than are in stock.\n",
    "OLTP databases are optimized to read, write and\n",
    "update single rows of data to ensure that business processes go smoothly.\n",
    "But they aren't necessarily designed to read many rows together. \n",
    "\n",
    "- OLAP\n",
    "\n",
    "OLAP stands for online analytical processing.\n",
    "This is a tool that has been optimized for analysis in addition to processing and\n",
    "can analyze data from multiple databases.\n",
    "OLAP systems pull data from multiple sources at one time to analyze data and\n",
    "provide key business insights. \n",
    "\n",
    "an OLAP system could\n",
    "pull data about customer purchases from multiple data warehouses.\n",
    "In order to create personalized home pages for customers based on their preferences.\n",
    "OLAP database systems enable organizations to address their analytical needs from\n",
    "a variety of data sources.\n",
    "Depending on the data maturity of the organization, one of your first tasks as\n",
    "a BI professional could be to set up an OLAP system. \n",
    "\n",
    "- Row-Based \n",
    "\n",
    "Row based databases are organized by rows.\n",
    "Each row in a table is an instance or an entry in the database and\n",
    "details about that instance are recorded and organized by column.\n",
    "\n",
    "This means that if you wanted the average profit of all sales over the last five\n",
    "years from the bookstore database. You would have to pull each row from those years even if you don't need all of\n",
    "the information contained in those rows. \n",
    "\n",
    "- Columnar\n",
    "\n",
    "databases organized by columns.\n",
    "They're used in data warehouses because they are very useful for\n",
    "analytical queries.\n",
    "Columnar databases process data quickly,\n",
    "only retrieving information from specific columns. \n",
    "\n",
    "n our average profit of all sales, example, with a columnar database,\n",
    "you could choose to specifically pull the sales column instead of years worth\n",
    "of rows. \n",
    "\n",
    "- Single-home\n",
    "\n",
    "Single-home databases are databases where all the data is stored in the same\n",
    "physical location.\n",
    "This is less common for organizations dealing with large data sets.\n",
    "And will continue to become rarer as more and\n",
    "more organizations move their data storage to online and cloud providers. \n",
    "\n",
    "- Distributed Databases\n",
    "\n",
    "distributed databases are collection of data systems distributed across\n",
    "multiple physical locations.\n",
    "Think about them like telephone books: it's not actually possible to keep all\n",
    "the telephone numbers in the world in one book, it would be enormous.\n",
    "So instead, the phone numbers are broken up by location and\n",
    "across multiple books in order to make them more manageable. \n",
    "\n",
    "- Combined Systems\n",
    "\n",
    "our database systems that store and\n",
    "analyze data in the same place.\n",
    "This is a more traditional setup because it enables users to access all of\n",
    "the data that needs to stay in the system long-term.\n",
    "But it can become unwieldy as more data is added. Like the name implies,\n",
    "separated storage and\n",
    "computing systems are databases where less relevant data is stored remotely.\n",
    "And the relevant data is stored locally for analysis. \n",
    "\n",
    "For example, if you have a lot of data but only a few people are querying it,\n",
    "you don't need as much computing power, which can save resources. \n",
    "\n",
    "\n",
    "### Table Comparisons\n",
    "\n",
    "*OLAP vs OLTP*\n",
    "Database Technology | Description | Use\n",
    "---|---|---\n",
    "OLAP | Online Analytical Processing (OLAP) systems are databases that have been primarily optimized for analysis. | Provide user access to data from a variety of source systems, Used by BI and other data professionals to support decision-making processes, Analyze data from multiple databases, Draw actionable insights from data delivered to reporting tables\n",
    "OLTP | Online Transaction Processing (OLTP) systems are databases that have been optimized for data processing instead of analysis. | Store transaction data Used by customer-facing employees or customer self-service applications Read, write, and update single rows of data Act as source systems that data pipelines can be pulled from for analysis\n",
    "\n",
    "*Distributed vs Single-Homed*\n",
    "Database Technology | Description | Use\n",
    "---|---|---\n",
    "Distributed|Distributed databases are collections of data systems distributed across multiple physical locations.|Easily expanded to address increasing or larger scale business needs, Accessed from different networks, Easier to secure than a single-homed database system\n",
    "Single-homed|Single-homed databases are databases where all of the data is stored in the same physical location.|Data stored in a single location is easier to access and coordinate cross-team, Cuts down on data redundancy, Cheaper to maintain than larger, more complex systems\n",
    "\n",
    "\n",
    "*Seperated Storage and Compute vs Combined*\n",
    "Database Technology | Description | Use\n",
    "---|---|---\n",
    "Separated storage and compute| Separated storage and computing systems are databases where less relevant data is stored remotely, and relevant data is stored locally for analysis.| Run analytical queries more efficiently because the system only needs to process the most relevant data, Scale computation resources and storage systems separately based on your organization’s custom needs\n",
    "Combined storage and compute| Combined systems are database systems that store and analyze data in the same place.| Traditional setup that allows users to access all possible data at once, Storage and computation resources are linked, so resource management is straightforward\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the Right Database\n",
    "\n",
    "### Elements of Database Schema\n",
    "\n",
    "a data warehouse is a specific type of database\n",
    "that consolidates data from multiple source systems for\n",
    "data consistency, accuracy and efficient access.\n",
    "Data warehouses are used to support data driven decision making.\n",
    "Often these systems are managed by data warehousing specialists but\n",
    "BI professionals may help design them when it comes to designing a data warehouse.\n",
    "\n",
    "Typically the shape of data refers to the rows and\n",
    "columns of tables within the warehouse and how they are laid out.\n",
    "The volume of data currently and in the future also changes how the warehouse is\n",
    "designed and the model the warehouse will follow includes all of the tools and\n",
    "constraints of the system, such as the database itself and\n",
    "any analysis tools that will be incorporated into the system. \n",
    "\n",
    "they're interested in\n",
    "measuring store profitability and\n",
    "website traffic in order to evaluate the effectiveness of annual promotions. \n",
    "\n",
    "The primary business process is sales.\n",
    "We could have a sales table that includes information such as quantity ordered,\n",
    "total based amount, total tax amount, total discounts and total net amount.\n",
    "These are the facts as a refresher. \n",
    "\n",
    "For instance, store, customer product promotion,\n",
    "time, stock or currency could all be dimensions. \n",
    "\n",
    "here are several dimension tables all connected to a fact table at the center\n",
    "and this means we just created a star schema.\n",
    "With this model, you can answer the specific question,\n",
    "the effectiveness of annual promotions and\n",
    "also generate a dashboard with other KPIs and drill down reports. \n",
    "\n",
    "\n",
    "In this case, we started with the businesses specific needs,\n",
    "looked at the data dimensions we had and\n",
    "organize them into tables that formed relationships.\n",
    "Those relationships helped us determine that a star schema will be the most useful\n",
    "way to organize this data warehouse. \n",
    "\n",
    "### Making Useful Database Schema\n",
    "\n",
    "Logical data modeling.\n",
    "This involves representing different tables\n",
    "in the physical data model.\n",
    "Decisions have to be made about\n",
    "how a system will implement that model. \n",
    "\n",
    "\n",
    "it's important to consider the schema\n",
    "early on in any BI project.\n",
    "There are four elements a database schema should include. \n",
    "\n",
    "1. Relevant Data\n",
    "\n",
    "2. Names and data types for each column\n",
    "\n",
    "3. Consistent Formatting\n",
    "\n",
    "4. Unique Keys for every database entry and object\n",
    "\n",
    "it's often\n",
    "necessary for a BI professional to add\n",
    "new information to an existing schema if\n",
    "the current schema can't\n",
    "answer a specific business question.\n",
    "If the business wants to know\n",
    "which customer service employee\n",
    "responded the most to requests,\n",
    "we would need to add that information to\n",
    "the data warehouse and update the schema accordingly.\n",
    "The schema also needs to include names and\n",
    "data types for each column\n",
    "in each table within the database.\n",
    "Imagine if you didn't organize your kitchen drawers,\n",
    "it would be really difficult to find anything if\n",
    "all of your utensils were just thrown together.\n",
    "Instead, you probably have\n",
    "a specific place where you keep\n",
    "your spoons, forks and knives.\n",
    "\n",
    "\n",
    "For example, imagine we have\n",
    "two transactional systems that\n",
    "we're combining into one database.\n",
    "One tracks the promotion sent to users,\n",
    "and the other track sales to customers.\n",
    "In the source systems,\n",
    "the marketing system that tracks\n",
    "promotions could have a user ID column,\n",
    "while the sale system has customer ID instead.\n",
    "To be consistent in our warehouse schema,\n",
    "we'll want to use just one of these columns. \n",
    "\n",
    "1. The relevant data: The schema describes how the data is modeled and shaped within the database and must encompass all of the data being described.\n",
    "\n",
    "2. Names and data types for each column: Include names and data types for each column in each table within the database.\n",
    "\n",
    "3. Consistent formatting: Ensure consistent formatting across all data entries. Every entry is an instance of the schema, so it needs to be consistent.\n",
    "\n",
    "4. Unique keys: The schema must use unique keys for each entry within the database. These keys build connections between the tables and enable users to combine relevant data from across the entire database.\n",
    "\n",
    "### Review Schema\n",
    "\n",
    "Francisco’s Electronics is launching an e-commerce store for its new home office product line. If it’s a success, company decision-makers plan to bring the rest of their products online as well. The company brought on Mia, a senior BI engineer, to help design its data warehouse. The database needed to store order data for analytics and reporting, and the sales manager needed to generate reports quickly to track the sales so that the success of the site can be determined.\n",
    "\n",
    "The sales_warehouse database schema contains five tables: Sales, Products, Users, Locations, and Orders, which are connected via keys. The tables contain five to eight columns (or attributes) that range in data type. The data types include varchar or char (or character), integer, decimal, date, text (or string), timestamp, bit, and other types depending on the database system chosen.\n",
    "\n",
    "- What kind of database schema is this? Why was this type of database selected? \n",
    "\n",
    "Mia designed the database with a star schema because Francisco’s Electronics is using this database for reporting and analytics. The benefits of star schema include simpler queries, simplified business reporting logic, query performance gains, and fast aggregations. \n",
    "\n",
    "- What naming conventions are used for the tables and fields? Are there any benefits of using these naming conventions? \n",
    "\n",
    "This schema uses a snake case naming convention. In snake case, underscores replace spaces and the first letter of each word is lowercase. Using a naming convention helps maintain consistency and improves database readability. Since snake case for tables and fields is an industry standard, Mia used it in the database.\n",
    "\n",
    "- What is the purpose of using the decimal fields in data elements? \n",
    "\n",
    "For fields related to money, there are potential errors when calculating prices, taxes, and fees. You might have values that are technically impossible, such as a value of  $0.001, when the smallest value for the United States dollar is one cent, or $0.01. To keep values consistent and avoid accumulated errors, Mia used a decimal(10,2) data type, which only keeps the last two digits after the decimal point. \n",
    "Note: Other numeric values, such as exchange rate and quantities, may need extra decimal places to minimize rounding differences in calculations. Also, other data types may be better suited for other fields. To track when an order is created (created_at), you can use a timestamp data type. For other fields with various text sizes, you can use varchar. \n",
    "\n",
    "- What is the purpose of each foreign and primary key in the database?\n",
    "\n",
    "Mia designed the Sales table with a primary key ID and included foreign keys in the other tables to reference the primary keys. The foreign keys must be the same data type as their corresponding primary keys. As you’ve learned, primary keys uniquely identify precisely one record on a table, and foreign keys establish integrity references from that primary key to records in other tables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Data Moves\n",
    "\n",
    "### ETL and Pipelines\n",
    "\n",
    "As a refresher, a data pipeline\n",
    "is a series of processes that transports\n",
    "data from different sources to\n",
    "their final destination for storage and analysis.\n",
    "This automates the flow of data from sources to targets\n",
    "while transforming the data to make it\n",
    "useful as soon as it reaches its destination.\n",
    "In other words, data pipelines are\n",
    "used to get data from point A to point B,\n",
    "automatically save time and\n",
    "resources and make data more accessible and useful. \n",
    "\n",
    "They automate the processes\n",
    "involved in extracting, transforming,\n",
    "combining, validating,\n",
    "and loading data for further analysis and visualization.\n",
    "Effective data pipelines also help\n",
    "eliminate errors and combat system latency. \n",
    "\n",
    "One of the most useful things about\n",
    "a data pipeline is that it can\n",
    "pull data from multiple sources,\n",
    "consolidate it, and then\n",
    "migrate it over to its proper destination.\n",
    "These sources can include relational databases,\n",
    "a website application with\n",
    "transactional data or an external data source.\n",
    "Usually, the pipeline has\n",
    "a push mechanism that enables it to ingest\n",
    "data from multiple sources in near\n",
    "real time or regular intervals.\n",
    "Once the data has been pulled into the pipeline,\n",
    "it can be loaded to its destination. \n",
    "\n",
    "Often while data is being moved from point A to point B,\n",
    "the pipeline is also transforming the data.\n",
    "Transformations include sorting, validation,\n",
    "and verification, making the data easier to analyze.\n",
    "This process is called the ETL system.\n",
    "ETL stands for extract, transform, and load. \n",
    "\n",
    "Let's say a business analyst has data in\n",
    "one place and needs to move it to another,\n",
    "that's where a data pipeline comes in.\n",
    "But a lot of the time,\n",
    "the structure of the source system isn't\n",
    "ideal for analysis which is why\n",
    "a BI professional wants to transform\n",
    "that data before it gets to the destination system\n",
    "and why having set database schemas\n",
    "already designed and ready to\n",
    "receive data is so important. \n",
    "\n",
    "Data pipelines in 3 stages\n",
    "\n",
    "1. ingesting the raw data\n",
    "\n",
    "2. processing and consolidating it into categories\n",
    "\n",
    "3. dumping the data into reporting tables that users can access\n",
    "\n",
    "Once we've determined what the stakeholders goal is,\n",
    "we can start thinking about what data\n",
    "we need the pipeline to ingest.\n",
    "In this case, we're going to\n",
    "want demographic data about the customers.\n",
    "Our stakeholders are interested in monthly reports.\n",
    "We can set up our pipeline to automatically\n",
    "pull in the data we want at monthly intervals.\n",
    "Once the data is ingested,\n",
    "we also want our pipeline to\n",
    "perform some transformations,\n",
    "so that it's clean and consistent\n",
    "once it gets delivered to our target tables. \n",
    "\n",
    "### ETL Process\n",
    "Like other pipelines, ETL processes work in\n",
    "stages and these stages are extract, transform, and load.\n",
    "Let's start with extraction. \n",
    "\n",
    "**Exctraction**\n",
    "- In this stage, the pipeline accesses\n",
    "a source systems and then read and\n",
    "collects the necessary data from within them.\n",
    "Many organizations store\n",
    "their data in transactional databases,\n",
    "such as OLTP systems,\n",
    "which are great for logging records\n",
    "or maybe the business uses flat files,\n",
    "for instance, HTML or log files.\n",
    "Either way, ETL makes the data useful for analysis by\n",
    "extracting it from its source and\n",
    "moving it into a temporary staging table. \n",
    "\n",
    "**Transformation**\n",
    "- The specific transformation activities\n",
    "depend on the structure and\n",
    "format of the destination\n",
    "and the requirement of the business case,\n",
    "but as you've learned,\n",
    "these transformations generally include validating,\n",
    "cleaning, and preparing the data for analysis.\n",
    "This stage is also when\n",
    "the ETL pipeline maps the datatypes from\n",
    "the sources to the target systems so\n",
    "the data fits the destination conventions. \n",
    "\n",
    "**Loading**\n",
    "- This is when data is delivered to its target destination.\n",
    "That could be a data warehouse, a data lake,\n",
    "or an analytics platform that\n",
    "works with direct data feeds.\n",
    "Note that once the data has been delivered,\n",
    "it can exist within multiple locations\n",
    "in multiple formats.\n",
    "For example, there could be\n",
    "a snapshot table that covers a week of\n",
    "data and a larger archive\n",
    "that has some of the same records.\n",
    "This helps ensure the historical data\n",
    "is maintained within\n",
    "the system while giving stakeholders\n",
    "focused, timely data,\n",
    "and if the business is interested in\n",
    "understanding and comparing average monthly sales,\n",
    "the data would be moved to\n",
    "an OLAP system that have\n",
    "been optimized for analysis queries. \n",
    "\n",
    "### Choosing The Right Tool\n",
    "\n",
    "\n",
    "KPIs let us know whether or not we're succeeding, so\n",
    "that we can adjust our processes to better reach objectives.\n",
    "For example, some financial KPIs are gross profit margin,\n",
    "net profit margin, and return on assets.\n",
    "Or some HR KPIs are rate of promotion and employee satisfaction. \n",
    "\n",
    "Next, depending on how your stakeholders want to view the data,\n",
    "there are different tools you can choose.\n",
    "Stakeholders might ask for graphs, static reports, or dashboards. \n",
    "\n",
    "Looker Studio, Microsoft, PowerBI and\n",
    "Tableau.\n",
    "Some others are Azura Analysis Service, CloudSQL, Pentaho,\n",
    "SSAS, and SSRS SQL Server, which all have reporting tools built in.\n",
    "\n",
    "### BI Tools\n",
    "Tool | Uses\n",
    "---|---\n",
    "Azure Analysis Service (AAS) | Connect to a variety of data sources, Build in data security protocols, Grant access and assign roles cross-team, Automate basic processes\n",
    "CloudSQL | Connect to existing MySQL, PostgreSQL or SQL Server databases,  Automate basic processes, Integrate with existing apps and Google Cloud services, including BigQuery, Observe database processes and make changes\n",
    "Looker Studio | Visualize data with customizable charts and tables, Connect to a variety of data sources, Share insights internally with stakeholders and online, Collaborate cross-team to generate reports, Use report templates to speed up your reporting\n",
    "Microsoft PowerBI |Connect to multiple data sources and develop detailed models, Create personalized reports, Use AI to get fast answers using conversational languages, Collaborate cross-team to generate and share insights on Microsoft applications\n",
    "Pentaho | Develop pipelines with a codeless interface, Connect to live data sources for updated reports, Establish connections to an expanded library, Access an integrated data science toolkit\n",
    "SSAS SQL Server | Access and analyze data across multiple online databases, Integrate with existing Microsoft services including BI and data warehousing tools and SSRS SQL Server, Use built-in reporting tools\n",
    "Tableau | Connect and visualize data quickly, Analyze data without technical programming languages, Connect to a variety of data sources including spreadsheets, databases, and cloud sources, Combine multiple views of the data in intuitive dashboards, Build in live connections with updating data sources\n",
    "\n",
    "### ETL Tools\n",
    "Tool | Uses\n",
    "---|---\n",
    "Apache Nifi |Connect a variety of data sources Access a web-based user interface Configure and change pipeline systems as needed Modify data movement through the system at any time\n",
    "Google DataFlow | Synchronize or replicate data across a variety of data sources, Identify pipeline issues with smart diagnostic features,  Use SQL to develop pipelines from the BigQuery UI, Schedule resources to reduce batch processing costs, Use pipeline templates to kickstart the pipeline creation process and share systems across your organization\n",
    "IBM InfoSphere Information Server |Integrate data across multiple systems, Govern and explore available data, Improve business alignment and processes, Analyze and monitor data from multiple data sources\n",
    "Microsoft SQL SIS |Connect data from a variety of sources integration, Use built-in transformation tools, Access graphical tools to create solutions without coding, Generate custom packages to address specific business needs\n",
    "Oracle Data Integrator | Connect data from a variety of sources, Track changes and monitor system performance with built-in features, Access system monitoring and drill-down capabilities, Reduce monitoring costs with access to built-in Oracle services\n",
    "Pentaho Data Integrator | Connect data from a variety of sources, Create codeless pipelines with drag-and-drop interface, Access dataflow templates for easy use, Analyze data with integrated tools\n",
    "Talend |Connect data from a variety of sources, Design, implement, and reuse pipeline from a cloud server, Access and search for data using integrated Talend services, Clean and prepare data with built-in tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataflow\n",
    "\n",
    "### Working in Google Cloud\n",
    "\n",
    "Google Data Flow is a serverless data-processing service that reads data\n",
    "from the source, transforms it, and writes it in the destination location.\n",
    "Dataflow creates pipelines with open source libraries which you can interact\n",
    "with using different languages including Python and SQL.\n",
    "Dataflow includes a selection of pre-built templates that you can customize or\n",
    "you can use SQL statements to build your own pipelines.\n",
    "The tool also includes security features to help keep your data safe.\n",
    "\n",
    "- Jobs \n",
    "\n",
    "When you first open the console, you will find the Jobs page. The Jobs page is where your current jobs are in your project space. There are also options to CREATE JOB FROM TEMPLATE or CREATE MANAGED DATA PIPELINE from this page, so that you can get started on a new project in your Dataflow console. This is where you will go anytime you want to start something new. \n",
    "\n",
    "- Pipelines\n",
    "\n",
    "Open the menu pane to navigate through the console and find the other pages in Dataflow. The Pipelines menu contains a list of all the pipelines you have created. If this is your first time using Dataflow, it will also display the processes you need to enable before you can start building pipelines. If you haven’t already enabled the APIs, click Fix All to enable the API features and set your location. \n",
    "\n",
    "- Workbench \n",
    "\n",
    "The Workbench section is where you can create and save shareable Jupyter notebooks with live code. This is helpful for first-time ETL tool users to check out examples and visualize the transformations. \n",
    "\n",
    "- Snapshots\n",
    "\n",
    "Snapshots save the current state of a pipeline to create new versions without losing the current state. This is useful when you are testing or updating current pipelines so that you aren’t disrupting the system. This feature also allows you to back up and recover old project versions. You may need to enable APIs to view the Snapshots page; you will learn more about APIs in an upcoming activity. \n",
    "\n",
    "- SQL Workspace\n",
    "\n",
    "Finally, the SQL Workspace is where you interact with your Dataflow jobs, connect to BigQuery functionality, and write necessary SQL queries for your pipelines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n",
    "### Coding With Python\n",
    "\n",
    "\n",
    "A programming language is a system of words and\n",
    "symbols used to write instructions that computers follow.\n",
    "There are lots of different programming languages,\n",
    "but Python was specifically developed to enable users to\n",
    "write commands in fewer lines than most other languages. \n",
    "\n",
    "Python is a general purpose programming language\n",
    "that can be applied to a variety of contexts.\n",
    "In business intelligence, it's used to connect to\n",
    "a database system to read and modify files.\n",
    "It can also be combined with\n",
    "other software tools to develop\n",
    "pipelines and it can\n",
    "even process big data and perform calculations. \n",
    "\n",
    "First, it is primarily object-oriented and interpreted.\n",
    "Let's first understand what it\n",
    "means to be object-oriented.\n",
    "Object-oriented programming languages are\n",
    "modeled around data objects.\n",
    "These objects are chunks of\n",
    "code that capture certain information.\n",
    "Basically, everything in the system is an object,\n",
    "and once data has been captured within the code,\n",
    "it's labeled and defined by the system so that\n",
    "it can be used again later\n",
    "without having to re-enter the data. \n",
    "\n",
    "Now, let's consider the fact that\n",
    "Python is an interpreted language.\n",
    "Interpreted languages are programming languages\n",
    "that use an interpreter;\n",
    "typically another program to\n",
    "read and execute coded instructions.\n",
    "This is different from a compiled programming language,\n",
    "which compiles coded instructions\n",
    "that are executed directly by the target machine.\n",
    "One of the biggest differences between\n",
    "these two types of programming languages is that\n",
    "the compiled code executed by\n",
    "the machine is almost impossible for humans to read. \n",
    "\n",
    "A notebook is an interactive,\n",
    "editable programming environment\n",
    "for creating data reports.\n",
    "This can be a great way to build\n",
    "dynamic reports for stakeholders.\n",
    "Python is a great tool to have in your BI toolbox.\n",
    "There's even an option to use\n",
    "Python commands in Google Dataflow. \n",
    "\n",
    "### Elements of Python\n",
    "\n",
    "- Python is open source and freely available to the public.\n",
    "\n",
    "- It is an interpreted programming language, which means it uses another program to read and execute coded instructions.\n",
    "\n",
    "- Data is stored in data frames, similar to R.\n",
    "\n",
    "- In BI, Python can be used to connect to a database system to work with files.\n",
    "\n",
    "- It is primarily object-oriented.\n",
    "\n",
    "- Formulas, functions, and multiple libraries are readily available.\n",
    "\n",
    "- A community of developers exists for online code support.\n",
    "\n",
    "- Python uses simple syntax for straightforward coding.\n",
    "\n",
    "- It integrates with cloud platforms including Google Cloud, Amazon Web Services, and Azure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery\n",
    "\n",
    "### Scenario\n",
    "\n",
    "- The problem  \n",
    "\n",
    "Consider a scenario in which a BI professional, Aviva, is working for a fictitious coffee shop chain. Each year, the cafes offer a variety of seasonal menu items. Company leaders are interested in identifying the most popular and profitable items on their seasonal menus so that they can make more confident decisions about pricing; strategic promotion; and retaining, expanding, or discontinuing menu items.\n",
    "The solution\n",
    "\n",
    "- Data extraction\n",
    "\n",
    "In order to obtain the information the stakeholders are interested in, Aviva begins extracting the data. The data extraction process includes locating and identifying relevant data, then preparing it to be transformed and loaded. To identify the necessary data, Aviva implements the following strategies:\n",
    "\n",
    "- Meet with key stakeholders\n",
    "\n",
    "Aviva leads a workshop with stakeholders to identify their objectives. During this workshop, she asks stakeholders questions to learn about their needs:\n",
    "\n",
    "1. What information needs to be obtained from the data (for instance, performance of different menu items at different restaurant locations)?\n",
    "\n",
    "2. What specific metrics should be measured (sales metrics, marketing metrics, product performance metrics)?\n",
    "\n",
    "3. What sources of data should be used (sales numbers, customer feedback, point of sales)?\n",
    "\n",
    "4. Who needs access to this data (management, market analysts)?\n",
    "\n",
    "5. How will key stakeholders use this data (for example, to determine which items to include on upcoming menus, make pricing decisions)?\n",
    "\n",
    "- Observe teams in action\n",
    "\n",
    "Aviva also spends time observing the stakeholders at work and asking them questions about what they’re doing and why. This helps her connect the goals of the project with the organization’s larger initiatives. During these observations, she asks questions about why certain information and activities are important for the organization.\n",
    "\n",
    "- Organize data in BigQuery\n",
    "\n",
    "Once Aviva has completed the data extraction process, she transforms the data she’s gathered from different stakeholders and loads it into BigQuery. Then she uses BigQuery to design a target table to organize the data. The target table helps Aviva unify the data. She then uses the target table to develop a final dashboard for stakeholders to review. \n",
    "\n",
    "- The results\n",
    "\n",
    "When stakeholders review the dashboard, they are able to identify several key findings about the popularity and profitability of items on their seasonal menus. For example, the data indicates that many peppermint-based products on their menus have decreased in popularity over the past few years, while cinnamon-based products have increased in popularity. This finding leads stakeholders to decide to retire three of their peppermint-based drinks and bakery items. They also decide to add a selection of new cinnamon-based offerings and launch a campaign to promote these items. \n",
    "\n",
    "\n",
    "\n",
    "**Data extraction**\n",
    "\n",
    "Data extraction is the process of taking data from a source system, such as a database or a SaaS, so that it can be delivered to a destination system for analysis. You might recognize this as the first step in an ETL (extract, transform, and load) pipeline. There are three primary ways that pipelines can extract data from a source in order to deliver it to a target table:\n",
    "\n",
    "- Update notification: The source system issues a notification when a record has been updated, which triggers the extraction.\n",
    "\n",
    "- Incremental extraction: The BI system checks for any data that has changed at the source and ingests these updates.\n",
    "\n",
    "- Full extraction: The BI system extracts a whole table into the target database system.\n",
    "\n",
    "Once data is extracted, it must be loaded into target tables for use. In order to drive intelligent business decisions, users need access to data that is current, clean, and usable. This is why it is important for BI professionals to design target tables that can hold all of the information required to answer business questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GCLOUDPROJECT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m \u001b[39mimport\u001b[39;00m bigquery\n\u001b[0;32m----> 3\u001b[0m client \u001b[39m=\u001b[39m bigquery\u001b[39m.\u001b[39mClient(project\u001b[39m=\u001b[39m GCLOUDPROJECT,\n\u001b[1;32m      4\u001b[0m                          credentials\u001b[39m=\u001b[39mGCLOUDAUTH)\n\u001b[1;32m      6\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39mSELECT\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[39m    address,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mLIMIT 10;\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     19\u001b[0m query_job \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mquery(query) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'GCLOUDPROJECT' is not defined"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "client = bigquery.Client(project= GCLOUD,\n",
    "                         credentials=GCLOUDAUTH)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    address,\n",
    "    COUNT(address) AS number_of_trees\n",
    "FROM\n",
    "    bigquery-public-data.san_francisco_trees.street_trees\n",
    "WHERE\n",
    "    address != \"null\"\n",
    "GROUP BY address\n",
    "ORDER BY number_of_trees DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query) \n",
    "\n",
    "print(\"The query data:\")\n",
    "for row in query_job:\n",
    "    # Row values can be accessed by field name or index.\n",
    "    print(\"Address = {}, Number of Trees={}\".format(row[0], row[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google-Cert-Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
