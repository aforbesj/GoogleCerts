{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Optimize ETL Processes\n",
    "\n",
    "You’ll learn about optimization techniques including ETL quality testing, data schema validation, business rule verification, and general performance testing. You’ll also explore data integrity and learn how built-in quality checks defend against potential problems. Finally, you’ll focus on verifying business rules and general performance testing to make sure pipelines meet the intended business need.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Discover strategies to create an ETL process that works to meet organizational and stakeholder needs and how to maintain an ETL process efficiently.\n",
    "\n",
    "- Introduce tools used in ETL\n",
    "\n",
    "- Understand the primary goals of ETL quality testing.\n",
    "\n",
    "- Understand the primary goals of data schema validation.\n",
    "\n",
    "- Develop ETL quality testing and data schema validation best practices.\n",
    "\n",
    "- Identify and implement appropriate test scenarios and checkpoints for QA on data pipelines.\n",
    "\n",
    "- Explain different methods for QA data in the pipeline.\n",
    "\n",
    "- Create performance testing scenarios and measure performance throughout the pipeline.\n",
    "\n",
    "- Verify business rules.\n",
    "\n",
    "- Perform general performance testing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Pipelines and ETL Processes\n",
    "\n",
    "### Elements of Quality Testing\n",
    "\n",
    "n BI, quality testing is a process of checking\n",
    "data for defects in order to prevent system failures.\n",
    "The goal is to ensure\n",
    "the pipeline continues to work properly.\n",
    "Quality testing can be time-consuming,\n",
    "but it's extremely important\n",
    "for an organization's workflow.\n",
    "Quality testing involves seven validation elements: completeness, consistency,\n",
    "conformity, accuracy,\n",
    "redundancy, integrity, and timeliness. \n",
    "\n",
    "- Completeness: Does the data contain all of the desired components or measures?\n",
    "\n",
    "- Consistency: Is the data compatible and in agreement across all systems?\n",
    "\n",
    "- Conformity: Does the data fit the required destination format?\n",
    "\n",
    "- Accuracy: Does the data conform to the actual entity being measured or described?\n",
    "\n",
    "- Redundancy: Is only the necessary data being moved, transformed, and stored for use?\n",
    "\n",
    "- Timeliness: Is the data current?\n",
    "\n",
    "- Integrity: Is the data accurate, complete, consistent, and trustworthy? (Integrity is influenced by the previously mentioned qualities.)\n",
    "\n",
    "There are also some common issues you can protect against within your system to ensure the incoming data doesn’t cause errors or other large-scale problems in your database system:\n",
    "\n",
    "- Check data mapping: Does the data from the source match the data in the target database?\n",
    "\n",
    "- Check for inconsistencies: Are there inconsistencies between the source system and the target system?\n",
    "\n",
    "- Check for inaccurate data: Is the data correct and does it reflect the actual entity being measured?\n",
    "\n",
    "- Check for duplicate data: Does this data already exist within the target system?\n",
    "\n",
    "To address these issues and ensure your data meets all seven elements of quality testing, you can build intermediate steps into your pipeline that check the loaded data against known parameters.\n",
    "\n",
    "For example, to ensure the timeliness of the data, you can add a checkpoint that determines if that data matches the current date; if the incoming data fails this check, there’s an issue upstream that needs to be flagged.\n",
    "\n",
    "Using SQL Queries and Logging them allows for a better understanding of how to access the issues.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Schema Vaildation\n",
    "\n",
    "### Data Dictionary and Lineage\n",
    "\n",
    "Schema validation is a process to ensure that\n",
    "the source system data schema\n",
    "matches the target database data schema.\n",
    "As you're learning, if the schemas don't align,\n",
    "this can cause system failures\n",
    "that are very difficult to fix.\n",
    "Building schema validation into your workflow,\n",
    "is important to prevent these issues.\n",
    "Database tools offer\n",
    "various schema validation options that can be used\n",
    "to check incoming data against\n",
    "the destination schema requirements. \n",
    "\n",
    "Schema validation properties should ensure three things.\n",
    "The keys are still valid after transformation.\n",
    "The table relationships have been preserved,\n",
    "and the conventions are consistent across the database.\n",
    "\n",
    "\n",
    "A data dictionary is a collection of\n",
    "information that describes the content,\n",
    "format and structure of data objects within a database,\n",
    "as well as their relationships.\n",
    "You might also hear this referred\n",
    "to as a metadata repository. \n",
    "\n",
    "\n",
    "Data lineage describes a process\n",
    "of identifying the origin of data,\n",
    "where it has moved throughout the system,\n",
    "and how it has transformed over time.\n",
    "This is useful because if you do get an error,\n",
    "you can track the lineage of that piece of data,\n",
    "and understand what happened\n",
    "along the way to cause the problem.\n",
    "Then, you can put standards in\n",
    "place to avoid the same issue in the future. \n",
    "\n",
    "### Schema Validation\n",
    "\n",
    "**Common issues for schema validation**\n",
    "\n",
    "- The keys are still valid: Primary and foreign keys build relationships between tables in relational databases. These keys should continue to function after you have moved data from one system into another.\n",
    "\n",
    "- The table relationships have been preserved: The keys help preserve the relationships used to connect the tables so that keys can still be used to connect tables. It’s important to make sure that these relationships are preserved or that they are transformed to match the target schema.\n",
    "\n",
    "- The conventions are consistent: The conventions for incoming data must be consistent with the target database’s schema. Data from outside sources might use different conventions for naming columns in tables– it’s important to align these before they’re added to the target system.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Rules and Performance Testing\n",
    "\n",
    "### Business Rules\n",
    "\n",
    "In BI,\n",
    "A business rule is a statement that creates a restriction on specific parts of\n",
    "a database.\n",
    "For example, a shipping database might impose a business rule that states\n",
    "shipping dates can't come before order dates.\n",
    "This prevents order dates and shipping dates from being mixed up and\n",
    "causing errors within this system. \n",
    "\n",
    "Basically verification involves ensuring that data imported into the target\n",
    "database complies with business rules on top of that.\n",
    "These rules are important pieces of knowledge that help a BI professional\n",
    "understand how a business and\n",
    "its processes function. \n",
    "\n",
    "Earlier, you learned about some database performance considerations you can check for when a database starts slowing down. Here is a quick checklist of those considerations:\n",
    "\n",
    "- Queries need to be optimized\n",
    "\n",
    "- The database needs to be fully indexed\n",
    "\n",
    "- Data should be defragmented\n",
    "\n",
    "- There must be enough CPU and memory for the system to process requests\n",
    "\n",
    "These general performance tests are really important– that’s how you know your database can handle data requests for your organization without any problems! But when it comes to database performance testing while considering your ETL process, there is another important check you should make: testing the table, column, row counts, and Query Execution Plan.\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "For example, let’s say the company you work for has a database that manages purchase order requests entered by employees. Purchase orders over $1,000 dollars need manager approval. In order to automate this process, you can impose a ruleset on the database that automatically delivers requests over $1,000 to a reporting table pending manager approval. Other business rules that may apply in this example are: prices must be numeric values (data type should be integer); or for a request to exist, a reason is mandatory (table field may not be null).\n",
    "\n",
    "In order to fulfill this business requirement, there are three rules at play in this system:\n",
    "\n",
    "1. Order requests under $1,000 are automatically delivered to the approved product order requests table\n",
    "\n",
    "2. Requests over $1,000 are automatically delivered to the requests pending approval table\n",
    "\n",
    "3. Approved requests are automatically delivered to the approved product order requests table\n",
    "\n",
    "These rules inherently affect the shape of this database system to cater to the needs of this particular organization.\n",
    "\n",
    "### Defending Against Known Issues\n",
    "\n",
    "Pipelines can have many different stages of processing. These stages, or layers, help ensure that the data is collected, aggregated, transformed, and staged in the most effective and efficient way. For example, it’s important to make sure you have all the data you need in one place before you start cleaning it to ensure that you don’t miss anything. There are usually four layers to this process: staging, harmonization, validation, and reconciliation. After these four layers, the data is brought into its target database and an error handling report summarizes each step of the process.\n",
    "\n",
    "**Staging Layer**\n",
    "\n",
    "First, the original data is brought from the source systems and stored in the staging layer. In this layer, Arsha ran the following defensive checks:\n",
    "\n",
    "- Compared the number of records received and stored\n",
    "\n",
    "- Compared rows to identify if extra records were created or records were lost\n",
    "\n",
    "- Checked important fields, such as amounts, dates, and IDs\n",
    "\n",
    "Arsha moved the mismatched records to the error handling report. She included each unconverted source record, the date and time of its first processing, its last retry date and time, the layer where the error happened, and a message describing the error. By collecting these records, Arsha was able to find and fix the origin of the problems. She marked all of the records that moved to the next layer as “processed.”\n",
    "\n",
    "**Harmonization layer**\n",
    "\n",
    "The harmonization layer is where data normalization routines and record enrichment are performed. This ensures that data formatting is consistent across all the sources. To harmonize the data, Arsha ran the following defensive checks:\n",
    "\n",
    "- Standardized the date format\n",
    "\n",
    "- Standardized the currency\n",
    " \n",
    "- Standardized uppercase and lowercase stylization\n",
    "\n",
    "- Formatted IDs with leading zeros\n",
    "\n",
    "- Split date values to store the year, month, and day in separate columns\n",
    "\n",
    "- Applied conversion and priority rules from the source systems\n",
    "\n",
    "When a record couldn’t be harmonized, she moved it to Error Handling. She marked all of the records that moved to the next layer as “processed.”\n",
    "\n",
    "**Validations layer**\n",
    "\n",
    "The validations layer is where business rules are validated. As a reminder, a business rule is a statement that creates a restriction on specific parts of a database. These rules are developed according to the way an organization uses data. Arsha ran the following defensive checks:\n",
    "\n",
    "- Ensured that values in the “department” column were not null, since “department” is a crucial dimension\n",
    "\n",
    "- Ensured that values in the “service type” column were within the authorized values to be processed\n",
    "\n",
    "- Ensured that each billing record corresponded to a valid processed contract\n",
    "\n",
    "Again, when a record couldn’t be harmonized, she moved it to error handling. She marked all the records that moved to the next layer as “processed.”\n",
    "\n",
    "**Reconciliation layer**\n",
    "\n",
    "The reconciliation layer is where duplicate or illegitimate records are found. Here, Arsha ran defensive checks to find the following types of records:\n",
    "\n",
    "- Slow-changing dimensions\n",
    "\n",
    "- Historic records\n",
    "\n",
    "- Aggregations\n",
    "\n",
    "As with the previous layers, Arsha moved the records that didn't pass the reconciliation rules to Error Handling. After this round of defensive checks, she brought the processed records into the BI and Analytics database (OLAP).\n",
    "\n",
    "**Error handling reporting and analysis**\n",
    "\n",
    "After completing the pipeline and running the defensive checks, Arsha made an error handling report to summarize the process. The report listed the number of records from the source systems, as well as how many records were marked as errors or ignored in each layer. The end of the report listed the final number of processed records.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "Accuracy: An element of quality testing used to confirm that data conforms to the actual entity being measured or described\n",
    "\n",
    "Business rule: A statement that creates a restriction on specific parts of a database\n",
    "\n",
    "Completeness: An element of quality testing used to confirm that data contains all desired components or measures\n",
    "\n",
    "Conformity: An element of quality testing used to confirm that data fits the required destination format\n",
    "\n",
    "Consistency: An element of quality testing used to confirm that data is compatible and in agreement across all systems\n",
    "\n",
    "Data dictionary: A collection of information that describes the content, format, and structure of data objects within a database, as well as their relationships\n",
    "\n",
    "Data lineage: The process of identifying the origin of data, where it has moved throughout the system, and how it has transformed over time\n",
    "\n",
    "Data mapping: The process of matching fields from one data source to another\n",
    "\n",
    "Integrity: An element of quality testing used to confirm that data is accurate, complete, consistent, and trustworthy throughout its life cycle\n",
    "\n",
    "Quality testing: The process of checking data for defects in order to prevent system failures; it involves the seven validation elements of completeness, consistency, conformity, accuracy, redundancy, integrity, and timeliness\n",
    "\n",
    "Redundancy: An element of quality testing used to confirm that no more data than necessary is moved, transformed, or stored\n",
    "\n",
    "Schema validation: A process to ensure that the source system data schema matches the target database data schema\n",
    "\n",
    "Timeliness: An element of quality testing used to confirm that data is current\n",
    "\n",
    "\n",
    "### Terms and definitions from previous weeks\n",
    "\n",
    "A\n",
    "\n",
    "Application programming interface (API): A set of functions and procedures that integrate computer programs, forming a connection that enables them to communicate \n",
    "\n",
    "Applications software developer: A person who designs computer or mobile applications, generally for consumers\n",
    "\n",
    "Attribute: In a dimensional model, a characteristic or quality used to describe a dimension\n",
    "\n",
    "B\n",
    "\n",
    "Business intelligence (BI): Automating processes and information channels in order to transform relevant data into actionable insights that are easily available to decision-makers\n",
    "\n",
    "Business intelligence governance: A process for defining and implementing business intelligence systems and frameworks within an organization\n",
    "\n",
    "Business intelligence monitoring: Building and using hardware and software tools to easily and rapidly analyze data and enable stakeholders to make impactful business decisions\n",
    "\n",
    "Business intelligence stages: The sequence of stages that determine both BI business value and organizational data maturity, which are capture, analyze, and monitor\n",
    "\n",
    "Business intelligence strategy: The management of the people, processes, and tools used in the business intelligence process\n",
    "\n",
    "C\n",
    "\n",
    "Columnar database: A database organized by columns instead of rows\n",
    "\n",
    "Combined systems: Database systems that store and analyze data in the same place\n",
    "\n",
    "Compiled programming language: A programming language that compiles coded instructions that are executed directly by the target machine\n",
    "\n",
    "Contention: When two or more components attempt to use a single resource in a conflicting way\n",
    "\n",
    "D\n",
    "\n",
    "Data analysts: People who collect, transform, and organize data\n",
    "\n",
    "Data availability: The degree or extent to which timely and relevant information is readily accessible and able to be put to use\n",
    "\n",
    "Data governance professionals: People who are responsible for the formal management of an organization’s data assets\n",
    "\n",
    "Data integrity: The accuracy, completeness, consistency, and trustworthiness of data throughout its life cycle\n",
    "\n",
    "Data lake: A database system that stores large amounts of raw data in its original format until it’s needed\n",
    "\n",
    "Data mart: A subject-oriented database that can be a subset of a larger data warehouse\n",
    "\n",
    "Data maturity: The extent to which an organization is able to effectively use its data in order to extract actionable insights\n",
    "\n",
    "Data model: A tool for organizing data elements and how they relate to one another\n",
    "\n",
    "Data partitioning: The process of dividing a database into distinct, logical parts in order to improve query processing and increase manageability\n",
    "\n",
    "Data pipeline: A series of processes that transports data from different sources to their final destination for storage and analysis\n",
    "\n",
    "Data visibility: The degree or extent to which information can be identified, monitored, and integrated from disparate internal and external sources\n",
    "\n",
    "Data warehouse: A specific type of database that consolidates data from multiple source systems for data consistency, accuracy, and efficient access\n",
    "\n",
    "Data warehousing specialists: People who develop processes and procedures to effectively store and organize data\n",
    "\n",
    "Database migration: Moving data from one source platform to another target database\n",
    "\n",
    "Database performance: A measure of the workload that can be processed by a database, as well as associated costs\n",
    "\n",
    "Deliverable: Any product, service, or result that must be achieved in order to complete a project\n",
    "\n",
    "Developer: A person who uses programming languages to create, execute, test, and troubleshoot software applications\n",
    "\n",
    "Dimension (data modeling): A piece of information that provides more detail and context regarding a fact\n",
    "\n",
    "Dimension table: The table where the attributes of the dimensions of a fact are stored\n",
    "\n",
    "Design pattern: A solution that uses relevant measures and facts to create a model in support of business needs\n",
    "\n",
    "Dimensional model: A type of relational model that has been optimized to quickly retrieve data from a data warehouse\n",
    "\n",
    "Distributed database: A collection of data systems distributed across multiple physical locations\n",
    "\n",
    "E\n",
    "\n",
    "ELT (extract, load, and transform): A type of data pipeline that enables data to be gathered from data lakes, loaded into a unified destination system, and transformed into a useful format \n",
    "\n",
    "ETL (extract, transform, and load): A type of data pipeline that enables data to be gathered from source systems, converted into a useful format, and brought into a data warehouse or other unified destination system\n",
    "\n",
    "Experiential learning: Understanding through doing\n",
    "\n",
    "F\n",
    "\n",
    "Fact: In a dimensional model, a measurement or metric\n",
    "\n",
    "Fact table: A table that contains measurements or metrics related to a particular event\n",
    "\n",
    "Foreign key: A field within a database table that is a primary key in another table (Refer to primary key)\n",
    "\n",
    "Fragmented data: Data that is broken up into many pieces that are not stored together, often as a result of using the data frequently or creating, deleting, or modifying files\n",
    "\n",
    "Functional programming language: A programming language modeled around functions\n",
    "\n",
    "G\n",
    "\n",
    "Google DataFlow: A serverless data-processing service that reads data from the source, transforms it, and writes it in the destination location\n",
    "\n",
    "I\n",
    "\n",
    "Index: An organizational tag used to quickly locate data within a database system\n",
    "\n",
    "Information technology professionals: People who test, install, repair, upgrade, and maintain hardware and software solutions\n",
    "\n",
    "Interpreted programming language: A programming language that uses an interpreter, typically another program, to read and execute coded instructions\n",
    "\n",
    "Iteration: Repeating a procedure over and over again in order to keep getting closer to the desired result\n",
    "\n",
    "K\n",
    "\n",
    "Key performance indicator (KPI): A quantifiable value, closely linked to business strategy, which is used to track progress toward a goal\n",
    "\n",
    "L\n",
    "\n",
    "Logical data modeling: Representing different tables in the physical data model\n",
    "\n",
    "M\n",
    "\n",
    "Metric: A single, quantifiable data point that is used to evaluate performance\n",
    "\n",
    "O\n",
    "\n",
    "Object-oriented programming language: A programming language modeled around data objects\n",
    "\n",
    "OLAP (Online Analytical Processing) system: A tool that has been optimized for analysis in addition to processing and can analyze data from multiple databases\n",
    "\n",
    "OLTP (Online Transaction Processing) database: A type of database that has been optimized for data processing instead of analysis\n",
    "\n",
    "Optimization: Maximizing the speed and efficiency with which data is retrieved in order to ensure high levels of database performance\n",
    "\n",
    "P\n",
    "\n",
    "Portfolio: A collection of materials that can be shared with potential employers\n",
    "\n",
    "Primary key: An identifier in a database that references a column or a group of columns in which each row uniquely identifies each record in the table (Refer to foreign key)\n",
    "\n",
    "Project manager: A person who handles a project’s day-to-day steps, scope, schedule, budget, and resources\n",
    "\n",
    "Project sponsor: A person who has overall accountability for a project and establishes the criteria for its success\n",
    "\n",
    "Python: A general purpose programming language\n",
    "\n",
    "Q\n",
    "\n",
    "Query plan: A description of the steps a database system takes in order to execute a query\n",
    "\n",
    "R\n",
    "\n",
    "Resources: The hardware and software tools available for use in a database system\n",
    "\n",
    "Response time: The time it takes for a database to complete a user request\n",
    "\n",
    "Row-based database: A database that is organized by rows\n",
    "\n",
    "S\n",
    "\n",
    "Separated storage and computing systems: Databases where data is stored remotely, and relevant data is stored locally for analysis\n",
    "\n",
    "Single-homed database: Database where all of the data is stored in the same physical location\n",
    "\n",
    "Snowflake schema: An extension of a star schema with additional dimensions and, often, subdimensions\n",
    "\n",
    "Star schema: A schema consisting of one fact table that references any number of dimension tables\n",
    "\n",
    "Strategy: A plan for achieving a goal or arriving at a desired future state\n",
    "\n",
    "Subject-oriented: Associated with specific areas or departments of a business\n",
    "\n",
    "Systems analyst: A person who identifies ways to design, implement, and advance information systems in order to ensure that they help make it possible to achieve business goals\n",
    "\n",
    "Systems software developer: A person who develops applications and programs for the backend processing systems used in organizations\n",
    "\n",
    "T\n",
    "\n",
    "Tactic: A method used to enable an accomplishment\n",
    "\n",
    "Target table: The predetermined location where pipeline data is sent in order to be acted on\n",
    "\n",
    "Throughput: The overall capability of the database’s hardware and software to process requests\n",
    "\n",
    "Transferable skill: A capability or proficiency that can be applied from one job to another\n",
    "\n",
    "V\n",
    "\n",
    "Vanity metric: Data points that are intended to impress others, but are not indicative of actual performance and, therefore, cannot reveal any meaningful business insights\n",
    "\n",
    "W\n",
    "\n",
    "Workload: The combination of transactions, queries, data warehousing analysis, and system commands being processed by the database system at any given time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google-Cert-Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
